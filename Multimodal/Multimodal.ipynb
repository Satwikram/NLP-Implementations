{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14H7lNqNrzP8_fZBs3Kz15WvFb1ZFK_Rv",
      "authorship_tag": "ABX9TyOn4AAF/8Tt2De+3BGetM2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satwikram/NLP-Implementations/blob/main/Multimodal/Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Author: Satwik Ram K"
      ],
      "metadata": {
        "id": "13XyiToiXdYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "BWX0WHAAXlO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "!pip install pytesseract\n",
        "# !pip install pdf2image\n",
        "!apt-get install poppler-utils \n",
        "!apt install tesseract-ocr"
      ],
      "metadata": {
        "id": "uE1P4wHUXnBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Dependencies"
      ],
      "metadata": {
        "id": "z025H7dPXhQh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x0mDTo6pV2Mt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling1D, BatchNormalization, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import spacy\n",
        "from unicodedata import normalize\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gathering Dataset"
      ],
      "metadata": {
        "id": "85mxY7tgZ1iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Deceptive-Research/dataset.zip"
      ],
      "metadata": {
        "id": "4tlnnX9dYHyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Variables"
      ],
      "metadata": {
        "id": "o-tnPitoaXzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = (300, 300)\n",
        "deceptive_path = Path(\"/content/deceptive\")\n",
        "normal_path = Path(\"/content/Ads/\")\n",
        "uniq_labels = [\"Non Deceptive\", \"Deceptive\"]"
      ],
      "metadata": {
        "id": "txahp5lOaBiu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the Data"
      ],
      "metadata": {
        "id": "LgGfbNEjaj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
      ],
      "metadata": {
        "id": "1rEqkoyWafRj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = normalize(\"NFKD\", text) #Normalization\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\",\"\", text) #Remove Punc\n",
        "\n",
        "    text = \" \".join([token.lemma_ for token in nlp(text) if not token.is_stop])\n",
        "\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "uzgHlwQ8a9gU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_img(fname):\n",
        "\n",
        "  img = cv2.imread(fname)\n",
        "\n",
        "  img = cv2.resize(img, img_shape) \n",
        "\n",
        "  # Normalization\n",
        "  img = img/255.0\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "qsI053ElbN2I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OCR - Image to Text"
      ],
      "metadata": {
        "id": "bVquYz24kgS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(image):\n",
        "\n",
        "  txt = pytesseract.image_to_string(Image.open(image), lang=\"eng\")\n",
        "  txt = re.sub(\"[\\n]{2,}\", \"\\t\\t\", txt)\n",
        "  txt = re.sub(\"\\n\", \"\", txt)\n",
        "  txt = re.sub(\"\\t\\t\", \"\\n\", txt)\n",
        "\n",
        "  return txt"
      ],
      "metadata": {
        "id": "DLCILN-UkfRS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Extraction"
      ],
      "metadata": {
        "id": "FG3tIKSIdHgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image(path, target):\n",
        "\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "  y = []\n",
        "\n",
        "  for img in os.listdir(path):\n",
        "\n",
        "    _, tail = os.path.splitext(img)\n",
        "\n",
        "    if tail in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "\n",
        "      try:\n",
        "        \n",
        "        fname = f\"{path}/{img}\"\n",
        "\n",
        "        # OCR - Image to Text        \n",
        "        text = get_data(fname)\n",
        "\n",
        "        # Cleaning the text\n",
        "        text = clean_text(text)\n",
        "\n",
        "        if not text:\n",
        "          text = \"No Information\"\n",
        "          print(fname)\n",
        "\n",
        "        # Cleaning the Image\n",
        "        img = clean_img(fname)\n",
        "\n",
        "        X1.extend([img])\n",
        "        X2.extend([text])\n",
        "        y.extend([target])\n",
        "\n",
        "      except Exception as e: print(e)\n",
        "\n",
        "  return X1, X2, y"
      ],
      "metadata": {
        "id": "vBwvkHcVcxvM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1, X2, y = extract_image(deceptive_path, 1)"
      ],
      "metadata": {
        "id": "I9nOQPw5da6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2"
      ],
      "metadata": {
        "id": "q71cvLYB8fSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir(normal_path):\n",
        "  X1_t, X2_t, y_t = extract_image(f\"{normal_path}/{i}\", 0)\n",
        "  X1.extend(X1_t)\n",
        "  X2.extend(X2_t)\n",
        "  y.extend(y_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7XfZdCDd0BI",
        "outputId": "6e149922-dd0a-4fbb-ed6d-71654058a524"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Ads/19/11.png\n",
            "/content/Ads/19/13.png\n",
            "/content/Ads/19/15.png\n",
            "/content/Ads/11/11.png\n",
            "/content/Ads/10/15.png\n",
            "/content/Ads/10/12.png\n",
            "/content/Ads/4/13.png\n",
            "/content/Ads/4/15.png\n",
            "/content/Ads/15/13.png\n",
            "/content/Ads/3/11.png\n",
            "/content/Ads/7/14.png\n",
            "/content/Ads/7/13.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.array(X1)\n",
        "# X2 = np.array(X2)\n",
        "\n",
        "y = np.array(y, dtype=\"float32\")"
      ],
      "metadata": {
        "id": "bRCXQt4gd6Pj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1.shape, X2.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "G5P0rttpeA_y",
        "outputId": "b0fcfda8-c4fd-4549-c8ce-1a38c3b38c40"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-98bb6ed74c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "NVDCYMH__5Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = \"bert-base-uncased\"\n",
        "checkpoint = \"gpt2\"\n",
        "sequence_length = 256\n",
        "\n",
        "def tokenize(samples):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  if checkpoint == \"gpt2\" and tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  tokens = tokenizer(\n",
        "      samples,\n",
        "      max_length=sequence_length,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"np\"\n",
        "  )\n",
        "\n",
        "  return {\"input_ids\": tokens[\"input_ids\"].tolist(), \"attention_mask\": tokens[\"attention_mask\"].tolist()}"
      ],
      "metadata": {
        "id": "sWC6VFql_6zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "id": "SX_WrbcXAmot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}